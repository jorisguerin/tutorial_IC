{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Tutorial on Image Clustering\n",
    "\n",
    "## Part 2\n",
    "\n",
    "by Joris Gu√©rin"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "-"
    }
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(action='ignore', category=FutureWarning)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from sklearn.metrics import fowlkes_mallows_score as FM\n",
    "from sklearn.metrics import normalized_mutual_info_score as NMI\n",
    "\n",
    "import pickle\n",
    "import cv2\n",
    "\n",
    "from copy import deepcopy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def confusion_matrix(clusters, true_labels):\n",
    "    new_tl = deepcopy(true_labels)\n",
    "    l = list(set(true_labels))\n",
    "    for i in range(len(true_labels)):\n",
    "        for j in range(len(l)):\n",
    "            if true_labels[i] == l[j]:\n",
    "                new_tl[i] = j\n",
    "                \n",
    "    conf_mat = np.zeros([len(set(clusters)), len(set(new_tl))])\n",
    "    for i in range(len(clusters)):\n",
    "        conf_mat[clusters[i], new_tl[i]] += 1\n",
    "\n",
    "    return conf_mat\n",
    "\n",
    "def purity(clusters, true_labels):\n",
    "    conf_mat = confusion_matrix(clusters, true_labels)\n",
    "    sum_clu  = np.max(conf_mat, axis = 1)\n",
    "    sum_tot  = np.sum(sum_clu)\n",
    "\n",
    "    pur = sum_tot / len(clusters)\n",
    "\n",
    "    return pur"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "im_path, lab_path = \"./umist/raw\", \"./umist\"\n",
    "n_im = 575\n",
    "\n",
    "fi = open(\"%s/true_labels.txt\" % lab_path, \"r\")\n",
    "true_labels = fi.readlines()\n",
    "true_labels = np.array([int(lab.rstrip(\"\\n\")) for lab in true_labels])\n",
    "fi.close()\n",
    "\n",
    "raw_data = []\n",
    "for i in range(n_im):\n",
    "    raw_data.append(cv2.imread(\"%s/%s.png\" % (im_path, i), cv2.IMREAD_GRAYSCALE).flatten())\n",
    "raw_data = np.array(raw_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dimensionality reduction for data visualization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE on raw images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "\n",
    "tsne2D = TSNE(n_components = 2)\n",
    "rawData2D = tsne2D.fit_transform(raw_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rawData2D[:, 0], rawData2D[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(rawData2D[:, 0], rawData2D[:, 1], c = true_labels, cmap = plt.get_cmap(\"tab20b\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### t-SNE on VGG19 features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_path = \"./umist/cnn_features/\"\n",
    "\n",
    "feat_file = open(feat_path + \"vgg19_fc2\" + \".p\", \"rb\")\n",
    "feat_vgg19 = pickle.load(feat_file)\n",
    "feat_file.close()\n",
    "\n",
    "feat_vgg19_2D = tsne2D.fit_transform(feat_vgg19) \n",
    "\n",
    "plt.scatter(feat_vgg19_2D[:, 0], feat_vgg19_2D[:, 1], c = true_labels, cmap = plt.get_cmap(\"tab20b\"))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ensemble clustering with multiple CNN feature extractors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nets_list = [\n",
    "    \"densenet121_avg_pool\",\n",
    "    \"densenet169_avg_pool\",\n",
    "    \"densenet201_avg_pool\",\n",
    "    \"inceptionV3_avg_pool\",\n",
    "    \"inception_resnetV2_avg_pool\",\n",
    "    \"nasnet_global_average_pooling2d_1\",\n",
    "    \"resnet50_avg_pool\",\n",
    "    \"vgg16_fc2\",\n",
    "    \"vgg19_fc2\",\n",
    "    \"xception_avg_pool\"\n",
    "]\n",
    "\n",
    "agg20 = AgglomerativeClustering(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "feat_path = \"./umist/cnn_features/\"\n",
    "\n",
    "data = []\n",
    "for net in nets_list:\n",
    "    feat_file = open(feat_path + net + \".p\", \"rb\")\n",
    "    data.append(pickle.load(feat_file))\n",
    "    feat_file.close()\n",
    "\n",
    "    print(data[-1].shape)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Solve for each feature extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "preds = []\n",
    "for d in data:\n",
    "    preds.append(agg20.fit_predict(d))\n",
    "    print(purity(preds[-1], true_labels), NMI(preds[-1], true_labels), FM(preds[-1], true_labels))\n",
    "preds = np.array(preds)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numba as nb\n",
    "\n",
    "agg_ens = AgglomerativeClustering(20, affinity='precomputed', linkage=\"average\")\n",
    "\n",
    "@nb.jit\n",
    "def computeM(P):\n",
    "    n = len(P[0])\n",
    "    M = np.zeros([n, n])\n",
    "    for i in range(M.shape[0]):\n",
    "        for j in range(M.shape[1]):\n",
    "            for k in range(len(P)):\n",
    "                M[i, j] += int(P[k][i] == P[k][j])\n",
    "            M[i, j] /= len(P)\n",
    "    return M\n",
    "\n",
    "def getFinalScores(Partition):\n",
    "    M = computeM(Partition)\n",
    "\n",
    "    clusters = agg_ens.fit_predict(-M)\n",
    "\n",
    "    nmi = NMI(clusters, true_labels)\n",
    "    pur = purity(clusters, true_labels)\n",
    "    fm = FM(clusters, true_labels)\n",
    "    return pur, nmi, fm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final_scores = getFinalScores(preds)\n",
    "print(final_scores)\n",
    "\n",
    "purs = [purity(preds[i, :], true_labels) for i in range(10)]\n",
    "nmis = [NMI(preds[i, :], true_labels) for i in range(10)]\n",
    "fms = [FM(preds[i, :], true_labels) for i in range(10)]\n",
    "\n",
    "purs.append(0)\n",
    "purs.append(final_scores[0])\n",
    "nmis.append(0)\n",
    "nmis.append(final_scores[1])\n",
    "fms.append(0)\n",
    "fms.append(final_scores[2])\n",
    "\n",
    "print(\"Purity scores\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(range(len(purs)), purs, 0.4)\n",
    "ax.set_ylim(bottom = 0.5)\n",
    "plt.show()\n",
    "\n",
    "print(\"NMI scores\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(range(len(nmis)), nmis, 0.4)\n",
    "ax.set_ylim(bottom = 0.65)\n",
    "plt.show()\n",
    "\n",
    "print(\"FM scores\")\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "rects1 = ax.bar(range(len(fms)), fms, 0.4)\n",
    "ax.set_ylim(bottom = 0.35)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## JULE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.models import Sequential, Model\n",
    "from keras.layers import Dense, BatchNormalization, Activation, Input, concatenate\n",
    "from keras.regularizers import l2\n",
    "from keras import backend as K\n",
    "\n",
    "from utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generating data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = \"tab20b\"\n",
    "\n",
    "n1 = 50\n",
    "mu1 = np.array([2,5])\n",
    "sig1 = np.array([[3,0],[0,2]])\n",
    "groupe1 = np.random.multivariate_normal(mu1, sig1, n1)\n",
    "\n",
    "n2 = 50\n",
    "mu2 = np.array([7,10])\n",
    "sig2 = np.array([[1,0],[0,2]])\n",
    "groupe2 = np.random.multivariate_normal(mu2, sig2, n2)\n",
    "\n",
    "n3 = 50\n",
    "mu3 = np.array([9,2])\n",
    "sig3 = np.array([[3,0],[0,3]])\n",
    "groupe3 = np.random.multivariate_normal(mu3, sig3, n3)\n",
    "\n",
    "data = np.r_[groupe1, groupe2, groupe3]\n",
    "labels = np.array([0] * n1 + [1] * n2 + [2] * n3)\n",
    "\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels, cmap = cm)\n",
    "plt.show()\n",
    "plt.scatter(data[:, 0], data[:, 1], cmap = cm)\n",
    "plt.show()\n",
    "\n",
    "agg3 = AgglomerativeClustering(3)\n",
    "clu_agg = agg3.fit_predict(data)\n",
    "\n",
    "print(NMI(labels, clu_agg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_neighbors_pts = 20\n",
    "n_neighbors_clu = 5\n",
    "hlayer_shape = 100\n",
    "reg_weight = 0.01\n",
    "lrate = 0.001\n",
    "n_neg = 10\n",
    "unfold_rate = 0.9\n",
    "n_clusters_target = 3\n",
    "n_epochs = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, indices = get_NearestNeighbors(data, n_neighbors_pts)\n",
    "labels_cur = []\n",
    "labels_cur.append(0)\n",
    "labels_cur_table, labels_cur[-1] = initialize_clusters(indices, data.shape[0])\n",
    "#print(len(labels_cur))\n",
    "#print(sum([len(labels_cur_table[i]) for i in range(len(labels_cur_table))]))\n",
    "n_clusters = len(list(set(labels_cur[-1])))\n",
    "print(n_clusters)\n",
    "plt.scatter(data[:, 0], data[:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "network = Sequential()\n",
    "network.add(Dense(hlayer_shape, input_shape=(2,), activation='relu'))\n",
    "network.add(Dense(2, activation='linear', kernel_regularizer=l2(reg_weight)))\n",
    "\n",
    "data_cur = []\n",
    "data_cur.append(0)\n",
    "data_cur[-1] = network.predict(data)\n",
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.savefig('initial_network.pdf', format = 'pdf')\n",
    "plt.show()\n",
    "\n",
    "anchor_in = Input(shape=(2,))\n",
    "pos_in = Input(shape=(2, ))\n",
    "neg_in = Input(shape=(2, ))\n",
    "\n",
    "anchor_out = network(anchor_in)\n",
    "pos_out = network(pos_in)\n",
    "neg_out = network(neg_in)\n",
    "merged_vector = concatenate([anchor_out, pos_out, neg_out], axis=-1)\n",
    "\n",
    "trainable_model = Model(inputs=[anchor_in, pos_in, neg_in], outputs=merged_vector)\n",
    "optimizer = Adam(lr = lrate)\n",
    "trainable_model.compile(optimizer=optimizer, loss=triplet_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    a, p, n = make_triplets(labels_cur[-1], n_clusters, n_neg)\n",
    "    inputs = [data[a], data[p], data[n]]\n",
    "    trainable_model.fit(inputs, np.zeros([inputs[0].shape[0], 3]), batch_size=200, epochs=1, verbose=False)\n",
    "data_cur.append(network.predict(data))\n",
    "\n",
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, W = affinity_computation(data_cur[-1], n_neighbors_pts)\n",
    "A_us, A_s = compute_cluster_affinity(W, labels_cur_table)\n",
    "\n",
    "nClusters = len(labels_cur_table)\n",
    "print(nClusters)\n",
    "unfold_iter = np.ceil(nClusters * unfold_rate)\n",
    "unfold_iter_max = nClusters - n_clusters_target\n",
    "n_iter = int(np.min([unfold_iter, unfold_iter_max]))\n",
    "if n_iter > 0:\n",
    "    labels_cur_table = run_agglomerative_clustering(A_us, A_s, labels_cur_table, n_iter, n_neighbors_clu)\n",
    "labels_cur.append(convert_labels_tab(labels_cur_table))\n",
    "nClusters = len(labels_cur_table)\n",
    "print(nClusters)\n",
    "\n",
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    a, p, n = make_triplets(labels_cur[-1], n_clusters, n_neg)\n",
    "    inputs = [data[a], data[p], data[n]]\n",
    "    trainable_model.fit(inputs, np.zeros([inputs[0].shape[0], 3]), batch_size=200, epochs=1, verbose=False)\n",
    "data_cur.append(network.predict(data))\n",
    "\n",
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Merge labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, _, W = affinity_computation(data_cur[-1], n_neighbors_pts)\n",
    "A_us, A_s = compute_cluster_affinity(W, labels_cur_table)\n",
    "\n",
    "nClusters = len(labels_cur_table)\n",
    "print(nClusters)\n",
    "unfold_iter = np.ceil(nClusters * unfold_rate)\n",
    "unfold_iter_max = nClusters - n_clusters_target\n",
    "n_iter = int(np.min([unfold_iter, unfold_iter_max]))\n",
    "if n_iter > 0:\n",
    "    labels_cur_table = run_agglomerative_clustering(A_us, A_s, labels_cur_table, n_iter, n_neighbors_clu)\n",
    "labels_cur.append(convert_labels_tab(labels_cur_table))\n",
    "nClusters = len(labels_cur_table)\n",
    "print(nClusters)\n",
    "\n",
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(n_epochs):\n",
    "    a, p, n = make_triplets(labels_cur[-1], n_clusters, n_neg)\n",
    "    inputs = [data[a], data[p], data[n]]\n",
    "    trainable_model.fit(inputs, np.zeros([inputs[0].shape[0], 3]), batch_size=200, epochs=1, verbose=False)\n",
    "data_cur.append(network.predict(data))\n",
    "\n",
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels_cur[-1], cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Final Ground truth"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(data_cur[-1][:, 0], data_cur[-1][:, 1], c=labels, cmap = cm)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agg3 = AgglomerativeClustering(3)\n",
    "clu_agg = agg3.fit_predict(data)\n",
    "\n",
    "print(NMI(labels_cur[-1], labels))\n",
    "print(NMI(labels, clu_agg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
